{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "# dataset from https://www.kaggle.com/datasnaek/mbti-type\n",
    "mbti = pd.read_csv('mbti.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Returns a list of words that make up the text.\n",
    "    \n",
    "    Note: for simplicity, lowercase everything.\n",
    "    Requirement: Use regular expressions to satisfy this function\n",
    "    \n",
    "    Params: {text: String}\n",
    "    Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # fix regular expression to remove links\n",
    "    return re.findall('[a-z]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mbti(df):\n",
    "    tokenized = []\n",
    "    for idx, row in df.iterrows():\n",
    "        t = row['type']\n",
    "        text = tokenize(row['posts'])\n",
    "        tokenized.append((t, text))\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize_mbti(mbti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with mbti and tokens\n",
    "mbti_dict = {}\n",
    "for (a, b) in tokenized:\n",
    "    if a not in mbti_dict:\n",
    "        mbti_dict[a] = b\n",
    "    else:\n",
    "        mbti_dict[a] += b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with word and mbti counts\n",
    "word_dict = {}\n",
    "for key in mbti_dict:\n",
    "    word_set = set(mbti_dict.get(key))\n",
    "    for word in word_set:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_idf(word_dict, n_docs, min_df=1, max_df_ratio=1):\n",
    "  \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # initialization\n",
    "    idf_dict = {}\n",
    "    \n",
    "    # compute IDF\n",
    "    for word in word_dict:\n",
    "        word_df = word_dict[word] \n",
    "        word_df_ratio = word_df / n_docs\n",
    "        if word_df >= min_df and word_df_ratio < max_df_ratio:\n",
    "            # remove 1+ word_df in denominator (smoothing not needed)\n",
    "            idf_dict[word] = math.log2(n_docs / (word_df))\n",
    "            \n",
    "    return idf_dict\n",
    "\n",
    "idf_dict = compute_idf(word_dict, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_words_to_analyze(input_word_counts):\n",
    "    \"\"\"Returns a list of words to analyze in alphabetically sorted order\n",
    "        Params: {input_word_counts: Dict}\n",
    "        Returns: List\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    analyze_list = []\n",
    "    for word in input_word_counts:\n",
    "        if input_word_counts.get(word) > 1:\n",
    "            analyze_list.append(word)\n",
    "    return sorted(analyze_list)\n",
    "\n",
    "words_to_analyze = output_words_to_analyze(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tf_matrix = np.zeros((len(mbti_dict), len(words_to_analyze)))\n",
    "mbti_keys = list(mbti_dict.keys())\n",
    "\n",
    "for mbti in mbti_dict:\n",
    "    c = Counter(mbti_dict[mbti])\n",
    "    for index, word in enumerate(words_to_analyze):\n",
    "        count = c[word]\n",
    "        tf_matrix[mbti_keys.index(mbti)][index] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_query(input_query, words_to_analyze=words_to_analyze):\n",
    "    tokenize_query = tokenize(input_query.lower())\n",
    "    for token in tokenize_query:\n",
    "        if token not in words_to_analyze:\n",
    "            tokenize_query.remove(token)\n",
    "    return tokenize_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_doc_norms(index, idf, n_docs):\n",
    "    \"\"\"\n",
    "    Returns a numpy array of the norms of each MBTI.\n",
    "\n",
    "    Params: {index: Dictionary,\n",
    "           idf: Dictionary,\n",
    "           n_docs: Integer\n",
    "          }\n",
    "\n",
    "    Returns: Numpy Array\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    norms = np.zeros(n_docs)\n",
    "\n",
    "    # compute the norm of each MBTI and add to numpy array\n",
    "    for w, l in index.items():\n",
    "        if w in idf:\n",
    "            for mbti, tf in l:\n",
    "                calculation = math.pow((tf * idf[w]), 2)\n",
    "                norms[mbti] += calculation\n",
    "  \n",
    "    return np.sqrt(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = np.savetxt('doc_norms.txt', doc_norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(idf_dict, open('idf.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(inv_idx, n_docs, min_df=1, max_df_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with valid words as keys and their IDFs as values.\n",
    "\n",
    "    Params: {inv_idx: Dictionary,\n",
    "           n_docs: Integer,\n",
    "           min_df: Integer,\n",
    "           max_df_ratio: Float\n",
    "          }\n",
    "\n",
    "    Returns: Dictionary\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    idf_dict = {}\n",
    "\n",
    "    # compute IDF for each word in inverted index\n",
    "    for word in inv_idx:\n",
    "        word_df = len(inv_idx[word]) \n",
    "        word_df_ratio = word_df / n_docs\n",
    "        if word_df >= min_df and word_df_ratio < max_df_ratio:\n",
    "            idf_dict[word] = math.log2(n_docs / (1 + word_df))\n",
    "\n",
    "    return idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbti_tokenized(tokenized):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with MBTI keys and tokenized text values.\n",
    "\n",
    "    Params: {tokenized: List of Tuples}\n",
    "\n",
    "    Returns: Dictionary\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    mbti_dict = {}\n",
    "\n",
    "    # creates dictionary with MBTIs as keys with the tokenized text as values\n",
    "    for (a, b) in tokenized:\n",
    "        if a not in mbti_dict:\n",
    "            mbti_dict[a] = b\n",
    "        else:\n",
    "            mbti_dict[a] += b\n",
    "\n",
    "    return mbti_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti = pd.read_csv('mbti.csv')\n",
    "tokenized = tokenize_mbti(mbti)\n",
    "mbti_dict = mbti_tokenized(tokenized)\n",
    "inv_idx = build_inverted_index(mbti_dict)\n",
    "idf = compute_idf(inv_idx, 16)\n",
    "doc_norms = compute_doc_norms(inv_idx, idf, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(inv_idx, open('inv_idx.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
